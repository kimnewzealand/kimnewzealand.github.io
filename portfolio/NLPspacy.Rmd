
---
title : "Name Entity Recognition using Python spaCy in R"
output: html_document
Author: kimnewzealand
image : "img/portfolio/displacyNER1.jpg"
# showonlyimage : false
weight : 5
---


*23 May 2018*

I have been learning Natural Language Processing (NLP). Although there are many NLP R packages, this field appears to extensively use Python packages so I started my NLP journey in Python with a [Jupyter notebook](https://github.com/kimnewzealand/NLPexampleprojects/blob/master/Basic%2BText%2BMining%2BNLP%2BABC%2Bexample.ipynb).  The Python packages included are the research tool [NLTK](https://www.nltk.org/), [gensim](https://radimrehurek.com/gensim/) then the more recent [spaCy](https://spacy.io/).

The purpose of this post is the next step in the journey to produce a pipeline for text mining and Named Entity Recognition (NER) with the Python spaCy NLP Toolkit, in R. 

This is also made possible with the interface to Python, the reticulate R package, as an .

This will cover the following:

+ [Python initialisation in R](#python-initialisation)
+ [Load the data](#load-data)
+ [Data Summaries](#data-summary)
+ [Named entity recognition with spaCy](#named-entity-recognition-using-cleannlp)


This will not include advanced topic modeling and training annotation models in spaCy.

<br><hr>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,  
                      dev = "svglite",
                      fig.ext = ".svg"
)
```

## Load Packages

```{r load packages, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
# Packages for manipulating data
library(stringr)
library(lubridate)
# Packages for NLP
library(NLP)
# install.packages("openNLPmodels.en",repos = "http://datacube.wu.ac.at/", type = "source")
library(openNLP)
library(cleanNLP)
# cnlp_download_corenlp() # install the coreNLP Java back end 'CoreNLP' <http://stanfordnlp.github.io/CoreNLP/>
# Packages for Python interface
# Packages for Python
library(reticulate)
use_virtualenv("r-reticulate")
```

## Python Initialisation

We have loaded the required R packages including the [reticulate](https://rstudio.github.io/reticulate/articles/introduction.html) R package which provides an R interface to Python modules, classes, and functions.

As per the [cleanNLP R package documentation](https://cran.r-project.org/web/packages/cleanNLP/cleanNLP.pdf), we will load the [spaCy](https://spacy.io/) Python NLP backend.

```{r python config, echo=TRUE, message=FALSE, warning=FALSE}
# First set the executable. Note this needs to be set before any initialising
use_python("C:/Users/HOME/Anaconda3/python.exe") 
# py_available(initialize = TRUE) # should give TRUE
# Check Python configuration
py_config()
# Initialise the spaCy backend
cnlp_init_spacy()
```


## Load Data

The [A Million News Headlines dataset](https://www.kaggle.com/therohk/million-headlines/data) is easy to load with the read_csv function from the [readr](https://cran.r-project.org/web/packages/readr/index.html) R package.

> This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.) 

These files have been downloaded into a local directory first to agree to the terms of use.

```{r load data, include=FALSE}
setwd("~/Validly projects")
# Read in the check in data
abc <- read_csv("abcnews-date-text.csv")
```


## Data Summary

Let's take a look at the ABC headline data.

```{r data format, echo=TRUE}
# Change the date to a date format
abc$publish_date <- as.Date(as.character(abc$publish_date), format = '%Y%m%d')
# Add new columns for the year, month and day using the lubridate R package
abc <- abc %>%  
      mutate(year = lubridate::year(abc$publish_date),
             month = lubridate::month(abc$publish_date),
             day = lubridate::day(abc$publish_date))
# Take a look at the first rows in the  dataset
head(abc)
```

```{r count year}
# Count the number of articles per year
count_year <- abc %>% 
      group_by(year) %>% 
      count() %>% 
      arrange(desc(year))
count_year
# Count the number of articles per month
count_year_month <- abc %>% 
      filter(year==2017) %>% 
      group_by(year,month) %>% 
      count() %>% 
      arrange(desc(month))
count_year_month
```


Since this is a large dataset, we will use a subset of articles from January, 2017. 
```{r abc2017}
abc2017 <- abc[abc$year==2017 & abc$month==1,]$headline_text
# Remove the abc dataset to save space in memory
rm(abc)
```  


## Named Entity Recognition using CleanNLP

```{r create string}
# Collapse the list into one string
abctext <- paste(abc2017, collapse = " ")
abctext <- as.String(abctext)
writeLines(abctext, tf <- tempfile())
```

Annote the string of text using the cnlp_annotate function from [CleanNLP](https://cran.r-project.org/web/packages/cleanNLP/index.html). This annotate function performs the word tokensiation and parts of speech tagging steps.

```{r echo=TRUE}
# Annotate the string tf by running the annotation engine over the corpus of text
anno <- cnlp_annotate(tf)
# Summarise the tokens by parts of speech
cnlp_get_token(anno, include_root = FALSE) %>%
  group_by(upos) %>%
  summarize(posnum = n()) %>%
  arrange(desc(posnum)) 
# Summarise the count of entities
cnlp_get_entity(anno) %>%
  group_by(entity_type) %>%
  summarize(count = n())  %>%
  arrange(desc(count)) 
# Extract the entities of type GPE which is are geo-political entities such as city, state/province, and country
cnlp_get_entity(anno) %>%
  filter(entity_type == "GPE") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) 
# Extract the entities of type NORP which are Nationalities or religious or political groups.
cnlp_get_entity(anno) %>%
  filter(entity_type == "NORP") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
# Extract the entities of type PERSON which are People, including fictional.
cnlp_get_entity(anno) %>%
  filter(entity_type == "PERSON") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
# Extract the entities of type ORG which are Companies, agencies, institutions, etc.
cnlp_get_entity(anno) %>%
  filter(entity_type == "ORG") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
# Extract the entities of type MONEY which are Monetary values, including unit.
cnlp_get_entity(anno) %>%
  filter(entity_type == "MONEY") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
```


The other entities types can be viewed in the spaCy [documentation](https://spacy.io/usage/linguistic-features#section-named-entities).


See the [GitHub NLPexamples repo](https://github.com/kimnewzealand/NLPexampleprojects) for my other NLP projects in R and Python.

