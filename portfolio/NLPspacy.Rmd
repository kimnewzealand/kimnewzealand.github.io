
---
title : "Name Entity Recognition using Python spaCy in R"
output: html_document
Author: kimnewzealand
image : "img/portfolio/displacyNER1.jpg"
# showonlyimage : false
weight : 5
---


*23 May 2018*

Recently I have been working on an Natural Language Processing (NLP) client project. This field appears to extensively use Python packages so I used the opportunity to go on an NLP journey in Python, starting with a [Jupyter notebook](https://github.com/kimnewzealand/NLPexampleprojects/blob/master/Basic%2BText%2BMining%2BNLP%2BABC%2Bexample.ipynb).  The Python packages included here are the research tool [NLTK](https://www.nltk.org/), [gensim](https://radimrehurek.com/gensim/) then the more recent [spaCy](https://spacy.io/).

The purpose of this post is the next step in the journey to produce a pipeline for the NLP areas of text mining and Named Entity Recognition (NER) using the Python spaCy NLP Toolkit, in R. This is made possible with the interface to Python, the reticulate R package.

This post will cover the following:

+ [Python initialisation in R](#python-initialisation)
+ [Load the data](#load-data)
+ [Data Summaries](#data-summary)
+ [Named entity recognition with spaCy](#named-entity-recognition-using-cleannlp)


This will not however include advanced topic modeling and training annotation models in spaCy.

<br><hr>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,  
                      dev = "svglite",
                      fig.ext = ".svg"
)
```

## Load Packages

```{r load packages, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyverse)
# Packages for manipulating data
library(stringr)
library(lubridate)
# Packages for NLP
library(NLP)
# install.packages("openNLPmodels.en",repos = "http://datacube.wu.ac.at/", type = "source")
library(openNLP)
library(cleanNLP)
# cnlp_download_corenlp() # install the coreNLP Java back end 'CoreNLP' <http://stanfordnlp.github.io/CoreNLP/>
# Packages for Python interface
# Packages for Python
library(reticulate)
use_virtualenv("r-reticulate")
```

## Python Initialisation

We have loaded the required R packages including the [reticulate](https://rstudio.github.io/reticulate/articles/introduction.html) R package which provides an R interface to Python modules, classes, and functions.

As per the [cleanNLP R package documentation](https://cran.r-project.org/web/packages/cleanNLP/cleanNLP.pdf), we will load the [spaCy](https://spacy.io/) Python NLP backend.

```{r python config, echo=TRUE, message=FALSE, warning=FALSE}
# First set the executable. Note this needs to be set before any initialising
use_python("C:/Users/HOME/Anaconda3/python.exe") 
# py_available(initialize = TRUE) # should give TRUE
# Check Python configuration
py_config()
# Initialise the spaCy backend
cnlp_init_spacy()
```


## Load Data

The [A Million News Headlines dataset](https://www.kaggle.com/therohk/million-headlines/data) is easy to load with the read_csv function from the [readr](https://cran.r-project.org/web/packages/readr/index.html) R package.

> This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.) 

These files have been downloaded into a local directory first to agree to the terms of use.

```{r load data, include=FALSE}
setwd("~/Validly projects")
# Read in the check in data
abc <- read_csv("abcnews-date-text.csv")
```


## Data Summary

Let's take a look at the ABC headline data.

```{r data format, echo=TRUE}
# Change the date to a date format
abc$publish_date <- as.Date(as.character(abc$publish_date), format = '%Y%m%d')
# Add new columns for the year, month and day using the lubridate R package
abc <- abc %>%  
      mutate(year = lubridate::year(abc$publish_date),
             month = lubridate::month(abc$publish_date),
             day = lubridate::day(abc$publish_date))
# Take a look at the first rows in the  dataset
head(abc)
```

```{r count year}
# Count the number of articles per year
count_year <- abc %>% 
      group_by(year) %>% 
      count() %>% 
      arrange(desc(year))
count_year
# Count the number of articles per month
count_year_month <- abc %>% 
      filter(year==2017) %>% 
      group_by(year,month) %>% 
      count() %>% 
      arrange(desc(month))
count_year_month
```


Since this is a large dataset, we will use a subset of articles from January, 2017. 
```{r abc2017}
abc2017 <- abc[abc$year==2017 & abc$month==1,]$headline_text
# Remove the abc dataset to save space in memory
rm(abc)
```  


## Named Entity Recognition using CleanNLP and spaCy

```{r create string}
# Collapse the list into one string
abctext <- paste(abc2017, collapse = " ")
abctext <- as.String(abctext)
writeLines(abctext, tf <- tempfile())
```

Annotate the string of text using the cnlp_annotate function from [CleanNLP](https://cran.r-project.org/web/packages/cleanNLP/index.html). This annotate function performs the word tokenisation and parts of speech tagging steps.

```{r echo=TRUE}
# Annotate the string tf by running the annotation engine over the corpus of text
anno <- cnlp_annotate(tf)
# Summarise the tokens by parts of speech
cnlp_get_token(anno, include_root = FALSE) %>%
  group_by(upos) %>%
  summarize(posnum = n()) %>%
  arrange(desc(posnum)) 
# Summarise the count of entities
cnlp_get_entity(anno) %>%
  group_by(entity_type) %>%
  summarize(count = n())  %>%
  arrange(desc(count)) 
# Extract the entities of type GPE which is are geo-political entities such as city, state/province, and country
cnlp_get_entity(anno) %>%
  filter(entity_type == "GPE") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count)) 
# Extract the entities of type NORP which are Nationalities or religious or political groups.
cnlp_get_entity(anno) %>%
  filter(entity_type == "NORP") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
# Extract the entities of type PERSON which are People, including fictional.
cnlp_get_entity(anno) %>%
  filter(entity_type == "PERSON") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
# Extract the entities of type ORG which are Companies, agencies, institutions, etc.
cnlp_get_entity(anno) %>%
  filter(entity_type == "ORG") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
# Extract the entities of type MONEY which are Monetary values, including unit.
cnlp_get_entity(anno) %>%
  filter(entity_type == "MONEY") %>%
  group_by(entity) %>%
  summarize(count = n()) %>%
  arrange(desc(count))
```


The other entities types can be viewed in the spaCy [documentation](https://spacy.io/usage/linguistic-features#section-named-entities).


See the [GitHub NLPexamples repo](https://github.com/kimnewzealand/NLPexampleprojects) for my other NLP projects in R and Python.

